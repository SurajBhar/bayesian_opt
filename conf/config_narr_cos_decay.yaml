# conf/config_narrow.yaml

defaults:
  - _self_

datasets:
  pizza_dataset:
    name: pizza_dataset
    num_classes: 3
    train_dir: /home/sur06423/wacv_paper/wacv_paper/data/pizza_steak_sushi/train
    val_dir: /home/sur06423/wacv_paper/wacv_paper/data/pizza_steak_sushi/test

  daa:
    name: DAA
    num_classes: 34
    train_dir: /net/polaris/storage/deeplearning/sur_data/rgb_daa/split_0/train
    val_dir: /net/polaris/storage/deeplearning/sur_data/rgb_daa/split_0/val

  imstatefarm:
    name: imstatefarm
    num_classes: 10
    train_dir: /home/sur06423/wacv_paper/wacv_paper/data/imbalanced_v2/train
    val_dir: /home/sur06423/wacv_paper/wacv_paper/data/imbalanced_v2/validation

models:
  vit:
    type: vit_b_16
    in_features: 768
    mode: supervised

  dinov2:
    type: dinov2_vitb14
    in_features: 768
    mode: selfsupervised

experiment:
  dataset: daa  # Choose which dataset to use
  model: dinov2  # Choose which model to use

hyperparameters:
  optimizer:
    choices: ["AdamW"]  # Fixed to AdamW to narrow the search space
  learning_rate:
    lower: 4e-4 # 0.0004
    upper: 5e-2 # 0.05
    log: true  # Logarithmic scale for learning rate search
  weight_decay:
    lower: 1e-5 # 0.00005
    upper: 1e-3 # 0.003
    log: true  # Slightly higher max weight decay to help prevent overfitting
  scheduler:
    choices: ["CosineAnnealingLR"]  # CosineAnnealingLR
    T_max: 60  # First cycle duration
    eta_min: 0.0001  # Minimum learning rate in cosine decay
  epochs:
    choices: [60] # Total epochs for fine-tuning
  use_gpu:
    enabled: true  # Use GPU for training
  batch_size: 1024  # Batch size for training and validation


run_config:
  name: Dinov2_108_daa_split_0_BO_cos_LR
  storage_path: /net/polaris/storage/deeplearning/sur_data/ray_results
  checkpoint_frequency: 5

tuner:
  max_concurrent: 4  # Number of trials running in parallel
  num_samples: 35  # Total number of trials to sample
  resources_per_trial:
    gpu: 1
    cpu: 14

ray:
  num_cpus: 80
  num_gpus: 4
  include_dashboard: true
