# conf/config.yaml
# This is your main configuration file, which sets the defaults 
# and other settings.
# /net/polaris/storage/deeplearning/sur_data/ray_results
defaults:
  - _self_

datasets:
  pizza_dataset:
    name: pizza_dataset
    num_classes: 3
    train_dir: /home/sur06423/wacv_paper/wacv_paper/data/pizza_steak_sushi/train
    val_dir: /home/sur06423/wacv_paper/wacv_paper/data/pizza_steak_sushi/test

  daa:
    name: DAA
    num_classes: 34
    train_dir: /net/polaris/storage/deeplearning/sur_data/rgb_daa/split_0/train
    val_dir: /net/polaris/storage/deeplearning/sur_data/rgb_daa/split_0/val

  imstatefarm:
    name: imstatefarm
    num_classes: 10
    train_dir: /home/sur06423/wacv_paper/wacv_paper/data/imbalanced_v2/train
    val_dir: /home/sur06423/wacv_paper/wacv_paper/data/imbalanced_v2/validation

models:
  vit:
    type: vit_b_16
    in_features: 768
    mode: supervised

  dinov2:
    type: dinov2_vitb14
    in_features: 768
    mode: selfsupervised

experiment:
  dataset: daa  # Choose which dataset to use
  model: dinov2              # Choose which model to use

hyperparameters:
  optimizer: #This covers a spectrum from traditional momentum-based optimization (SGD) to more sophisticated adaptive methods (Adam, AdamW)
    choices: ["SGD", "Adam", "AdamW"]  # List of optimizers for BOHB to choose from
  learning_rate: # Using a logarithmic scale ensures that BOHB searches more densely in lower values, which is often where the optimal learning rate is found.
    lower: 1e-5
    upper: 3e-1
    log: true  # Indicates a logarithmic scale for learning rate
  weight_decay: # This parameter helps in preventing overfitting. A logarithmic scale is suitable for fine-tuning its influence on the training process.
    lower: 1e-6
    upper: 1e-3
    log: true  # Optional: use `log: true` if you want a logarithmic scale
  momentum: # This parameter helps to accelerate SGD in the relevant direction and dampens oscillations.
    lower: 0.5
    upper: 0.99
  scheduler: # These choices provide a variety of ways to adjust the learning rate during training, catering to different training dynamics.
    choices: ["StepLR", "CosineAnnealing", "ExponentialLR", "LinearLR"]
  step_size: # Determines the number of epochs before the learning rate is decayed.
    choices: [20, 30] # [10, 20]
  gamma: # This controls the factor by which the learning rate decreases each step or epoch
    lower: 0.1
    upper: 0.9
  T_max: # This sets the maximum number of epochs for one cycle, after which the learning rate resets.
    choices: [10, 20, 30] # [10, 20, 30]
  eta_min: 1e-5  # This is the minimum learning rate that the scheduler can assign.
  start_lr: null  # Null value for start_lr, can be used as a placeholder
  end_lr: 1e-5  # Ensures the learning rate decreases to a small value towards the end of training.
  epochs:
    choices: [80, 100]  # [60, 80, 100] Adequate range for thorough exploration of convergence behaviors over different lengths of training.
  use_gpu:
    enabled: true # Ensuring computations are done on GPU to accelerate the training process.
  batch_size: 2048  # 2x1024, Given the constraints and typical memory capacities, this is a reasonable batch size that balances the speed and stability of training.

run_config:
  name: Exp_001_Dinov2_BO_daa_split_0
  storage_path: /net/polaris/storage/deeplearning/sur_data/ray_results
  checkpoint_frequency: 2

tuner:
  max_concurrent: 4
  num_samples: 30
  resources_per_trial:
    gpu: 1
    cpu: 18

ray: # In case of Multi Node setup we are not going to use these configurations
  num_cpus: 140 # This is specific to a single node cluster in order to control it specifically
  num_gpus: 8
  include_dashboard: true